# Ollama_Test.py
import gradio as gr
import bs4
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings


# ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”
def load_and_retrieve_docs(url: str):
    loader = WebBaseLoader(
        web_paths=(url,),
        bs_kwargs=dict()  # BeautifulSoup ê¸°ë³¸ ì„¤ì •
    )
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

    return vectorstore.as_retriever()


# ë¬¸ì„œ í¬ë§·íŒ…
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# RAG ì²´ì¸ ë™ì‘
def rag_chain(url: str, question: str) -> str:
    retriever = load_and_retrieve_docs(url)
    retrieved_docs = retriever.invoke(question)

    context = format_docs(retrieved_docs)
    prompt = f"Question: {question}\n\nContext: {context}"

    response = ollama.chat(
        model='llama3',
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant. Check the url content and answer the question. Translate the answer in Korean with emoji."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )

    return response['message']['content']


# Gradio ì¸í„°í˜ì´ìŠ¤
iface = gr.Interface(
    fn=rag_chain,
    inputs=["text", "text"],
    outputs="text",
    title="ğŸ¦™ LLaMA 3 - RAG ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ",
    description="ì›¹í˜ì´ì§€ URLê³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ LLaMA 3ê°€ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ğŸ‡°ğŸ‡·âœ¨"
)

if __name__ == "__main__":
    iface.launch()
