{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi85mXeU7GmL"
   },
   "source": [
    "# 『5과목』 AI와 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJhL01TcVwVC"
   },
   "source": [
    "## 강화 학습과 게임 지능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix22DHQjVwVC"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmKRZ9LIVwVC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic' # Windows\n",
    "# matplotlib.rcParams['font.family'] = 'AppleGothic' # Mac\n",
    "matplotlib.rcParams['font.size'] = 15 # 글자 크기\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 한글 폰트 사용 시, 마이너스 글자가 깨지는 현상을 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X_Te3uu7IKF"
   },
   "source": [
    "### 다중 손잡이 밴딧 문제를 위한 랜덤 정책 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVIlHqTp7JGb",
    "outputId": "524feafe-13d6-4095-f848-9cafb1832456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률: ['0.3885', '0.1209', '0.5258', '0.5925', '0.2494']\n",
      "손잡이별 수익($): ['-427', '-1530', '103', '373', '-1029']\n",
      "순 수익($): -2510\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 다중 손잡이 밴딧 문제 설정\n",
    "arms_profit=[0.4, 0.12, 0.52, 0.6, 0.25]\n",
    "n_arms=len(arms_profit)\n",
    "\n",
    "n_trial=10000 # 손잡이를 당기는 횟수(에피소드 길이)\n",
    "\n",
    "# 손잡이 당기는 행위를 시뮬레이션하는 함수(handle은 손잡이 번호)\n",
    "def pull_bandit(handle):\n",
    "    q=np.random.random()\n",
    "    if q<arms_profit[handle]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# 랜덤 정책을 모방하는 함수\n",
    "def random_exploration():\n",
    "    episode=[]\n",
    "    num=np.zeros(n_arms) # 손잡이별로 당긴 횟수\n",
    "    wins=np.zeros(n_arms) # 손잡이별로 승리 횟수\n",
    "    for i in range(n_trial):\n",
    "        h=np.random.randint(0,n_arms)\n",
    "        reward=pull_bandit(h)\n",
    "        episode.append([h,reward])\n",
    "        num[h]+=1\n",
    "        wins[h]+=1 if reward==1 else 0\n",
    "    return episode, (num,wins)\n",
    "\n",
    "e,r=random_exploration()\n",
    "\n",
    "print(\"손잡이별 승리 확률:\", [\"%6.4f\"% (r[1][i]/r[0][i]) if r[0][i]>0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($):\",[\"%d\"% (2*r[1][i]-r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($):\",sum(np.asarray(e)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmU1xhvsVwVE"
   },
   "source": [
    "### ε-탐욕 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWSiYTux7RqK",
    "outputId": "db1c84eb-8394-4e8b-a3be-11ae82d818c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손잡이별 승리 확률: ['0.4092', '0.1247', '0.5371', '0.5902', '0.2560']\n",
      "손잡이별 수익($): ['-393', '-566', '203', '542', '-654']\n",
      "순 수익($): -868\n"
     ]
    }
   ],
   "source": [
    "# ε-탐욕을 구현하는 함수\n",
    "def epsilon_greedy(eps):\n",
    "    episode=[]\n",
    "    num=np.zeros(n_arms) # 손잡이별로 당긴 횟수\n",
    "    wins=np.zeros(n_arms) # 손잡이별로 승리 횟수\n",
    "    for i in range(n_trial):\n",
    "        r=np.random.random()\n",
    "        if(r<eps or sum(wins)==0): # 확률 eps로 임의 선택\n",
    "            h=np.random.randint(0,n_arms)\n",
    "        else:\n",
    "            prob=np.asarray([wins[i]/num[i] if num[i]>0 else 0.0 for i in range(n_arms)])\n",
    "            prob=prob/sum(prob)\n",
    "            h=np.random.choice(range(n_arms),p=prob)\n",
    "        reward=pull_bandit(h)\n",
    "        episode.append([h,reward])\n",
    "        num[h]+=1\n",
    "        wins[h]+=1 if reward==1 else 0\n",
    "    return episode, (num,wins)\n",
    "\n",
    "e,r=epsilon_greedy(0.1)\n",
    "\n",
    "print(\"손잡이별 승리 확률:\", [\"%6.4f\"% (r[1][i]/r[0][i]) if r[0][i]>0 else 0.0 for i in range(n_arms)])\n",
    "print(\"손잡이별 수익($):\",[\"%d\"% (2*r[1][i]-r[0][i]) for i in range(n_arms)])\n",
    "print(\"순 수익($):\",sum(np.asarray(e)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzpC2MlUnGNu",
    "outputId": "d4633342-74f7-494b-f96a-c0007abf27f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOEPFnqenxAd",
    "outputId": "62b8fdcf-4a2c-4b43-ebde-dcafecfcf1ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Surface(640x480x32 SW)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import pygame\n",
    "pygame.display.set_mode((640,480))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH_b-NZEmiEu"
   },
   "source": [
    "DeprecatedEnv: Environment version v0 for `FrozenLake` is deprecated. Please use `FrozenLake-v1` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVhF-XrX9AA6",
    "outputId": "f4c303a2-4adb-4ac8-9350-29579062c189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_trial):\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m# 행동을 취함(랜덤 선택)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     step_result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 보상을 받고 상태가 바뀜\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     obs \u001b[38;5;241m=\u001b[39m step_result[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# observation 값\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     reward \u001b[38;5;241m=\u001b[39m step_result[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# reward 값\u001b[39;00m\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m step_api_compatibility(\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_step_api:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    238\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    242\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\numpy\\__init__.py:427\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# 환경 불러오기\n",
    "env=gym.make(\"FrozenLake-v1\",is_slippery=False)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "n_trial=20\n",
    "# 에피소드 수집\n",
    "env.reset()\n",
    "episode=[]\n",
    "for i in range(n_trial):\n",
    "    action = env.action_space.sample() # 행동을 취함(랜덤 선택)\n",
    "    step_result = env.step(action) # 보상을 받고 상태가 바뀜\n",
    "    obs = step_result[0]  # observation 값\n",
    "    reward = step_result[1]  # reward 값\n",
    "    done = step_result[2]  # done 값\n",
    "    info = step_result[3]  # info 값\n",
    "    episode.append([action, reward, obs, done, info])  # 모든 값을 추가\n",
    "    # env.render() # 렌더링\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6sgQIY0VwVF"
   },
   "source": [
    "### 가치 반복 (Value Iteration) 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IB0lSWnjVwVF",
    "outputId": "d9662383-fc8f-4a6f-8c2c-158922dfccda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전이 확률 T:\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "보상 R:\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 4x4 FrozenLake 환경에서 상태와 행동의 개수\n",
    "num_states = 16\n",
    "num_actions = 4\n",
    "\n",
    "# 전이 확률 초기화\n",
    "T = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# 보상 초기화\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# 상태 전이 예시 (단순화된 형태로 일부만 보여줌)\n",
    "# 상(0), 하(1), 좌(2), 우(3) 행동에 대한 전이 확률 설정\n",
    "# 상(0) 행동\n",
    "T[0, 0, 0] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[1, 0, 1] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[4, 0, 0] = 1  # (1, 0) -> (0, 0)\n",
    "T[5, 0, 1] = 1  # (1, 1) -> (0, 1)\n",
    "\n",
    "# 하(1) 행동\n",
    "T[0, 1, 4] = 1  # (0, 0) -> (1, 0)\n",
    "T[1, 1, 5] = 1  # (0, 1) -> (1, 1)\n",
    "T[4, 1, 8] = 1  # (1, 0) -> (2, 0)\n",
    "T[5, 1, 9] = 1  # (1, 1) -> (2, 1)\n",
    "\n",
    "# 좌(2) 행동\n",
    "T[0, 2, 0] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[1, 2, 0] = 1  # (0, 1) -> (0, 0)\n",
    "T[4, 2, 4] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[5, 2, 4] = 1  # (1, 1) -> (1, 0)\n",
    "\n",
    "# 우(3) 행동\n",
    "T[0, 3, 1] = 1  # (0, 0) -> (0, 1)\n",
    "T[1, 3, 2] = 1  # (0, 1) -> (0, 2)\n",
    "T[4, 3, 5] = 1  # (1, 0) -> (1, 1)\n",
    "T[5, 3, 6] = 1  # (1, 1) -> (1, 2)\n",
    "\n",
    "# 보상 설정 예시\n",
    "R[0, 1, 4] = 0  # (0, 0)에서 하로 이동하여 (1, 0)이 됨 -> 보상 0\n",
    "R[0, 3, 1] = 0  # (0, 0)에서 우로 이동하여 (0, 1)이 됨 -> 보상 0\n",
    "R[14, 1, 15] = 1  # 목표 지점에 도달하면 보상 1\n",
    "\n",
    "print(\"전이 확률 T:\")\n",
    "print(T)\n",
    "\n",
    "print(\"보상 R:\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hYqbuJhqVwVF",
    "outputId": "dedec42c-b69c-4bfc-ae7c-aac2cb05de61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전이 확률 T:\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "보상 R:\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "최적 가치 함수 V:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 환경 설정\n",
    "states = range(16)  # 16개의 상태 (예: FrozenLake 4x4)\n",
    "actions = range(4)  # 4개의 행동 (상, 하, 좌, 우)\n",
    "gamma = 0.9  # 할인 인자\n",
    "epsilon = 1e-6  # 수렴 기준\n",
    "\n",
    "# 전이 확률과 보상 설정 (예시로 가정, 실제 환경에서는 다를 수 있음)\n",
    "# T = np.zeros((16, 4, 16))  # 전이 확률 T(s, a, s')\n",
    "# R = np.zeros((16, 4, 16))  # 보상 R(s, a, s')\n",
    "# 4x4 FrozenLake 환경에서 상태와 행동의 개수\n",
    "num_states = 16\n",
    "num_actions = 4\n",
    "\n",
    "# 전이 확률 초기화\n",
    "T = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# 보상 초기화\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# 상태 전이 예시 (단순화된 형태로 일부만 보여줌)\n",
    "# 상(0), 하(1), 좌(2), 우(3) 행동에 대한 전이 확률 설정\n",
    "# 상(0) 행동\n",
    "T[0, 0, 0] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[1, 0, 1] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[4, 0, 0] = 1  # (1, 0) -> (0, 0)\n",
    "T[5, 0, 1] = 1  # (1, 1) -> (0, 1)\n",
    "\n",
    "# 하(1) 행동\n",
    "T[0, 1, 4] = 1  # (0, 0) -> (1, 0)\n",
    "T[1, 1, 5] = 1  # (0, 1) -> (1, 1)\n",
    "T[4, 1, 8] = 1  # (1, 0) -> (2, 0)\n",
    "T[5, 1, 9] = 1  # (1, 1) -> (2, 1)\n",
    "\n",
    "# 좌(2) 행동\n",
    "T[0, 2, 0] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[1, 2, 0] = 1  # (0, 1) -> (0, 0)\n",
    "T[4, 2, 4] = 1  # 벽이므로 상태가 변하지 않음\n",
    "T[5, 2, 4] = 1  # (1, 1) -> (1, 0)\n",
    "\n",
    "# 우(3) 행동\n",
    "T[0, 3, 1] = 1  # (0, 0) -> (0, 1)\n",
    "T[1, 3, 2] = 1  # (0, 1) -> (0, 2)\n",
    "T[4, 3, 5] = 1  # (1, 0) -> (1, 1)\n",
    "T[5, 3, 6] = 1  # (1, 1) -> (1, 2)\n",
    "\n",
    "# 보상 설정 예시\n",
    "R[0, 1, 4] = 0  # (0, 0)에서 하로 이동하여 (1, 0)이 됨 -> 보상 0\n",
    "R[0, 3, 1] = 0  # (0, 0)에서 우로 이동하여 (0, 1)이 됨 -> 보상 0\n",
    "R[14, 1, 15] = 1  # 목표 지점에 도달하면 보상 1\n",
    "\n",
    "print(\"전이 확률 T:\")\n",
    "print(T)\n",
    "\n",
    "print(\"보상 R:\")\n",
    "print(R)\n",
    "\n",
    "# 초기 가치 함수\n",
    "V = np.zeros(16)\n",
    "\n",
    "# 가치 반복\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        v = V[s]\n",
    "        V[s] = max(sum(T[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
    "                       for s_prime in states) for a in actions)\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    if delta < epsilon:\n",
    "        break\n",
    "\n",
    "print(\"최적 가치 함수 V:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJesgZDPVwVF"
   },
   "source": [
    "### Q-learning 알고리즘을 사용하여 Q-값을 업데이트하는 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q9SJysp2VwVF",
    "outputId": "5af767c0-6b2f-40e9-87e0-148293f3c6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 Q-테이블:\n",
      "[[-4.46660087 -4.51176221 -4.4956603  -4.54851143]\n",
      " [-4.79659477 -4.76395001 -4.78297903 -4.79722439]\n",
      " [-4.83823706 -4.86310957 -4.82905312 -4.84320774]\n",
      " [-4.83510516 -4.81155304 -4.83434125 -4.81488675]\n",
      " [-4.73206297 -4.74794415 -4.71097079 -4.71399932]\n",
      " [-4.58070578 -4.63125618 -4.60550905 -4.61683806]\n",
      " [-4.44158527 -4.46110077 -4.42039216 -4.4195377 ]\n",
      " [-4.33801514 -4.32767603 -4.31381198 -4.30527794]\n",
      " [-4.18507352 -4.19033302 -4.16989379 -4.17930821]\n",
      " [-3.89047996 -3.94321715 -3.95811653 -3.89310765]\n",
      " [-3.80666914 -3.83524612 -3.80366033 -3.81289225]\n",
      " [-3.74168432 -3.71029271 -3.69993062 -3.71800141]\n",
      " [-3.35992596 -3.31463193 -3.31898454 -3.27461311]\n",
      " [-3.35497766 -3.2952054  -3.24451924 -3.24461855]\n",
      " [-3.26883405 -3.2450667  -3.40221574 -3.34475906]\n",
      " [-2.59793124 -2.67963711 -2.67502189 -2.63742641]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 환경 설정\n",
    "num_states = 16  # 상태의 개수 (예: FrozenLake 4x4)\n",
    "num_actions = 4  # 행동의 개수 (상, 하, 좌, 우)\n",
    "gamma = 0.9  # 할인 인자\n",
    "alpha = 0.1  # 학습률\n",
    "epsilon = 0.1  # 탐색률\n",
    "num_episodes = 1000  # 에피소드 수\n",
    "\n",
    "# Q-테이블 초기화\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# 전이 확률과 보상 설정 (단순화된 예시)\n",
    "T = np.zeros((num_states, num_actions, num_states))\n",
    "R = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# 환경 초기화 (상태 전이와 보상 설정)\n",
    "# (자세한 설정은 환경에 따라 다를 수 있음)\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        next_state = (s + a) % num_states  # 임의로 설정한 전이 규칙\n",
    "        T[s, a, next_state] = 1\n",
    "        R[s, a, next_state] = -1  # 임의의 보상\n",
    "\n",
    "# Q-learning 알고리즘\n",
    "for episode in range(num_episodes):\n",
    "    state = np.random.randint(0, num_states)  # 초기 상태 무작위 설정\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # 행동 선택 (탐욕적 선택과 탐색의 혼합)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(0, num_actions)  # 탐색\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])  # 탐욕적 선택\n",
    "\n",
    "        # 다음 상태 및 보상 관찰\n",
    "        next_state = np.argmax(T[state, action, :])\n",
    "        reward = R[state, action, next_state]\n",
    "\n",
    "        # Q-값 업데이트\n",
    "        best_next_action = np.argmax(Q[next_state, :])\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # 종료 조건 (예: 목표 상태에 도달하면 종료)\n",
    "        if state == num_states - 1:\n",
    "            done = True\n",
    "\n",
    "print(\"최적의 Q-테이블:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NTOq0hwVwVF"
   },
   "source": [
    "### 시간차(Temporal Difference) 학습과 Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S8QRWWyFVwVF",
    "outputId": "5a44f6fa-c460-485f-dc3b-26ccbc7e0ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.52   1.87   1.48]\n",
      " [  0.     -inf -15.05]\n",
      " [  -inf  13.31   -inf]]\n",
      "Optimal Policy: [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 데이터 설정 및 초기화\n",
    "nan = np.nan\n",
    "T = np.array([\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "    [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n",
    "])\n",
    "R = np.array([\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[0.0, 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "])\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "discount_factor = 0.99\n",
    "\n",
    "s = 0\n",
    "\n",
    "Q = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0\n",
    "\n",
    "# Q-learning 알고리즘 적용\n",
    "for iteration in range(n_iterations):\n",
    "    a = np.random.choice(possible_actions[s])\n",
    "    sp = np.random.choice(range(3), p=T[s, a])\n",
    "    reward = R[s, a, sp]\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    Q[s, a] = (1 - learning_rate) * Q[s, a] + learning_rate * (reward + discount_factor * np.max(Q[sp]))\n",
    "    s = sp\n",
    "\n",
    "# 최적의 정책 계산\n",
    "np.set_printoptions(precision=2)\n",
    "print(Q)\n",
    "optimal_policy = np.argmax(Q, axis=1)\n",
    "print(\"Optimal Policy:\", optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eI6JT6HTC9bx",
    "outputId": "87a7b55e-214b-4730-abd5-0b14485cf42d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m argmaxs\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margwhere(Q[s,:]\u001b[38;5;241m==\u001b[39mnp\u001b[38;5;241m.\u001b[39mamax(Q[s,:]))\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     18\u001b[0m a\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(argmaxs)\n\u001b[1;32m---> 19\u001b[0m s1,r,done,_\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m Q[s,a]\u001b[38;5;241m=\u001b[39mQ[s,a]\u001b[38;5;241m+\u001b[39mrho\u001b[38;5;241m*\u001b[39m(r\u001b[38;5;241m+\u001b[39mlamda\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(Q[s1,:])\u001b[38;5;241m-\u001b[39mQ[s,a]) \u001b[38;5;66;03m# 식 (9.18)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m s\u001b[38;5;241m=\u001b[39ms1\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m step_api_compatibility(\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_step_api:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    238\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    242\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\numpy\\__init__.py:427\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env=gym.make('FrozenLake-v1',is_slippery=False) # 환경 생성\n",
    "Q=np.zeros([env.observation_space.n,env.action_space.n]) # Q 배열 초기화\n",
    "\n",
    "rho=0.8 # 학습률\n",
    "lamda=0.99 # 할인율\n",
    "\n",
    "n_episode=2000\n",
    "length_episode=100\n",
    "\n",
    "# 최적 행동 가치 함수 찾기\n",
    "for i in range(n_episode):\n",
    "    s=env.reset() # 새로운 에피소드 시작\n",
    "    for j in range(length_episode):\n",
    "        argmaxs=np.argwhere(Q[s,:]==np.amax(Q[s,:])).flatten().tolist()\n",
    "        a=np.random.choice(argmaxs)\n",
    "        s1,r,done,_=env.step(a)\n",
    "        Q[s,a]=Q[s,a]+rho*(r+lamda*np.max(Q[s1,:])-Q[s,a]) # 식 (9.18)\n",
    "        s=s1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3eqrY0MToBQR",
    "outputId": "ea22cb47-f37b-46df-9a2b-a0380938ef28",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     q\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mreshape(s,[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m])) \u001b[38;5;66;03m# 신경망이 예측한 행동\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     a\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(q[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 66\u001b[0m s1,r,done,_\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m long_reward\u001b[38;5;241m<\u001b[39mmax_steps\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# 실패\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m step_api_compatibility(\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     step_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_step_api:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    238\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    242\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\DEV\\envs\\py3_10_tf\\lib\\site-packages\\numpy\\__init__.py:427\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "# 하이퍼 매개변수 설정\n",
    "rho=0.9 # 학습률\n",
    "lamda=0.99 # 할인율\n",
    "eps=0.9\n",
    "eps_decay=0.999\n",
    "batch_siz=64\n",
    "n_episode=100\n",
    "\n",
    "# 신경망을 설계해주는 함수\n",
    "def deep_network():\n",
    "    mlp=Sequential()\n",
    "    mlp.add(Dense(32,input_dim=env.observation_space.shape[0],activation='relu'))\n",
    "    mlp.add(Dense(32,activation='relu'))\n",
    "    mlp.add(Dense(env.action_space.n,activation='linear'))\n",
    "    mlp.compile(loss='mse',optimizer='Adam')\n",
    "    return mlp\n",
    "\n",
    "# DQN 학습\n",
    "def model_learning():\n",
    "    mini_batch=np.asarray(random.sample(D,batch_siz))\n",
    "    state=np.asarray([mini_batch[i,0] for i in range(batch_siz)])\n",
    "    action=mini_batch[:,1]\n",
    "    reward=mini_batch[:,2]\n",
    "    state1=np.asarray([mini_batch[i,3] for i in range(batch_siz)])\n",
    "    done=mini_batch[:,4]\n",
    "\n",
    "    target=model.predict(state)\n",
    "    target1=model.predict(state1)\n",
    "\n",
    "    for i in range(batch_siz):\n",
    "        if done[i]:\n",
    "            target[i][action[i]]=reward[i]\n",
    "        else:\n",
    "            target[i][action[i]]+=rho*((reward[i]+lamda*np.amax(target1[i]))-target[i][action[i]]) # Q 러닝(식 (9.19))\n",
    "    model.fit(state,target,batch_size=batch_siz,epochs=1,verbose=0)\n",
    "\n",
    "env=gym.make(\"CartPole-v0\")\n",
    "\n",
    "model=deep_network() # 신경망 생성\n",
    "D=deque(maxlen=2000) # 리플레이 메모리 초기화\n",
    "scores=[]\n",
    "max_steps=env.spec.max_episode_steps\n",
    "\n",
    "# 신경망 학습\n",
    "for i in range(n_episode):\n",
    "    s=env.reset()\n",
    "    long_reward=0\n",
    "\n",
    "    while True:\n",
    "        r=np.random.random()\n",
    "        eps=max(0.01,eps*eps_decay) # 엡시론을 조금씩 줄여나감\n",
    "        if(r<eps):\n",
    "            a=np.random.randint(0,env.action_space.n) # 랜덤 정책\n",
    "        else:\n",
    "            q=model.predict(np.reshape(s,[1,4])) # 신경망이 예측한 행동\n",
    "            a=np.argmax(q[0])\n",
    "        s1,r,done,_=env.step(a)\n",
    "        if done and long_reward<max_steps-1: # 실패\n",
    "            r=-100\n",
    "\n",
    "        D.append((s,a,r,s1,done))\n",
    "\n",
    "        if len(D)>batch_siz*3:\n",
    "            model_learning()\n",
    "\n",
    "        s=s1\n",
    "        long_reward+=r\n",
    "\n",
    "        if done:\n",
    "            long_reward=long_reward if long_reward==max_steps else long_reward+100\n",
    "            print(i,\"번째 에피소드의 점수:\",long_reward)\n",
    "            scores.append(long_reward)\n",
    "            break\n",
    "\n",
    "    if i>10 and np.mean(scores[-5:])>(0.95*max_steps):\n",
    "        break\n",
    "\n",
    "# 신경망 저장\n",
    "model.save(\"./cartpole_by_DQN.h5\")\n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,len(scores)+1),scores)\n",
    "plt.title('DQN scores for CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4uTHZeFpi3x",
    "outputId": "ad575b2a-dd69-41ee-beca-ff575f80cccc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# 신경망 블러옴\n",
    "model=load_model('./cartpole_by_DQN.h5')\n",
    "\n",
    "env=gym.make(\"CartPole-v0\")\n",
    "long_reward=0\n",
    "\n",
    "# CartPole 플레이\n",
    "s=env.reset()\n",
    "while True:\n",
    "    q=model.predict(np.reshape(s,[1,4])) # 신경망이 예측한 행동\n",
    "    a=np.argmax(q[0])\n",
    "    s1,r,done,_=env.step(a)\n",
    "    s=s1\n",
    "    long_reward+=r\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    if done:\n",
    "        print(\"에피소드의 점수:\",long_reward)\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "py3_10_tf",
   "language": "python",
   "name": "py3_10_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
