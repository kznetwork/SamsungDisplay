{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d711678-6a00-4d60-8bc7-e9c22874501f",
      "metadata": {
        "id": "1d711678-6a00-4d60-8bc7-e9c22874501f"
      },
      "source": [
        "# 『2과목』 Deep-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f829b180-aa44-4e1b-9549-6064183987d1",
      "metadata": {
        "id": "f829b180-aa44-4e1b-9549-6064183987d1"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988a9190-5086-4f5f-83b7-c27d50d39c2c",
      "metadata": {
        "id": "988a9190-5086-4f5f-83b7-c27d50d39c2c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['font.family'] = 'Malgun Gothic' # Windows\n",
        "# matplotlib.rcParams['font.family'] = 'AppleGothic' # Mac\n",
        "matplotlib.rcParams['font.size'] = 15 # 글자 크기\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False # 한글 폰트 사용 시, 마이너스 글자가 깨지는 현상을 해결"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfbc16a2-8f23-45b8-96a0-f8c7fc0b2f13",
      "metadata": {
        "id": "bfbc16a2-8f23-45b8-96a0-f8c7fc0b2f13"
      },
      "source": [
        "## 객체 검출-YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e458d01-b7fc-4f85-bd13-6bf31cad4811",
      "metadata": {
        "id": "2e458d01-b7fc-4f85-bd13-6bf31cad4811",
        "outputId": "3524a950-141d-4820-b423-1e19eb03d4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 6.25M/6.25M [00:00<00:00, 73.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 256x640 15 persons, 1 car, 3 buss, 553.9ms\n",
            "1: 256x640 15 persons, 1 car, 3 buss, 553.9ms\n",
            "Speed: 47.0ms preprocess, 553.9ms inference, 29.9ms postprocess per image at shape (1, 3, 256, 640)\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n",
        "\n",
        "# Define image file path\n",
        "image_folder = 'C:\\\\DEV\\\\SamsungElectronics_Gumi\\\\datasets\\\\YOLO\\\\'\n",
        "\n",
        "# Define image file names\n",
        "image_file_1 = 'im1.jpg'\n",
        "image_file_2 = 'im2.jpg'\n",
        "\n",
        "# Construct full image file paths\n",
        "image_path_1 = image_folder + image_file_1\n",
        "image_path_2 = image_folder + image_file_2\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "results = model([image_path_1, image_path_2])  # return a list of Results objects\n",
        "\n",
        "# Process results list\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871253a4-3e1e-46e5-8536-307a9e464255",
      "metadata": {
        "id": "871253a4-3e1e-46e5-8536-307a9e464255"
      },
      "source": [
        "## 박스형상 으로 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8aa6dc2-457e-4b00-bc19-3389d0084231",
      "metadata": {
        "scrolled": true,
        "id": "c8aa6dc2-457e-4b00-bc19-3389d0084231",
        "outputId": "29ccb17d-a6b8-4300-d3ad-8a840aeeba23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 256x640 15 persons, 162.5ms\n",
            "1: 256x640 15 persons, 162.5ms\n",
            "Speed: 1.3ms preprocess, 162.5ms inference, 35.4ms postprocess per image at shape (1, 3, 256, 640)\n",
            "Box coordinates:\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.7759])\n",
            "data: tensor([[504.1612, 181.5096, 560.9950, 337.8292,   0.7759,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[532.5781, 259.6694,  56.8338, 156.3197]])\n",
            "xywhn: tensor([[0.5201, 0.6641, 0.0555, 0.3998]])\n",
            "xyxy: tensor([[504.1612, 181.5096, 560.9950, 337.8292]])\n",
            "xyxyn: tensor([[0.4923, 0.4642, 0.5478, 0.8640]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.7362])\n",
            "data: tensor([[120.7271, 179.9198, 155.0141, 315.7537,   0.7362,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[137.8706, 247.8367,  34.2871, 135.8339]])\n",
            "xywhn: tensor([[0.1346, 0.6339, 0.0335, 0.3474]])\n",
            "xyxy: tensor([[120.7271, 179.9198, 155.0141, 315.7537]])\n",
            "xyxyn: tensor([[0.1179, 0.4602, 0.1514, 0.8076]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.7359])\n",
            "data: tensor([[7.8046e+02, 2.1255e+02, 8.5079e+02, 3.3896e+02, 7.3591e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[815.6260, 275.7527,  70.3301, 126.4117]])\n",
            "xywhn: tensor([[0.7965, 0.7052, 0.0687, 0.3233]])\n",
            "xyxy: tensor([[780.4609, 212.5469, 850.7910, 338.9586]])\n",
            "xyxyn: tensor([[0.7622, 0.5436, 0.8309, 0.8669]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.7201])\n",
            "data: tensor([[148.0024, 162.6864, 206.5362, 320.4906,   0.7201,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[177.2693, 241.5885,  58.5338, 157.8042]])\n",
            "xywhn: tensor([[0.1731, 0.6179, 0.0572, 0.4036]])\n",
            "xyxy: tensor([[148.0024, 162.6864, 206.5362, 320.4906]])\n",
            "xyxyn: tensor([[0.1445, 0.4161, 0.2017, 0.8197]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.6411])\n",
            "data: tensor([[566.3392, 191.5926, 622.6235, 336.1739,   0.6411,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[594.4813, 263.8832,  56.2842, 144.5813]])\n",
            "xywhn: tensor([[0.5805, 0.6749, 0.0550, 0.3698]])\n",
            "xyxy: tensor([[566.3392, 191.5926, 622.6235, 336.1739]])\n",
            "xyxyn: tensor([[0.5531, 0.4900, 0.6080, 0.8598]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.6227])\n",
            "data: tensor([[ 79.1016, 166.9388, 128.2383, 316.5261,   0.6227,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[103.6699, 241.7325,  49.1366, 149.5873]])\n",
            "xywhn: tensor([[0.1012, 0.6182, 0.0480, 0.3826]])\n",
            "xyxy: tensor([[ 79.1016, 166.9388, 128.2383, 316.5261]])\n",
            "xyxyn: tensor([[0.0772, 0.4270, 0.1252, 0.8095]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.5838])\n",
            "data: tensor([[288.3297, 141.1603, 328.3002, 197.8108,   0.5838,   0.0000]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[308.3149, 169.4855,  39.9705,  56.6505]])\n",
            "xywhn: tensor([[0.3011, 0.4335, 0.0390, 0.1449]])\n",
            "xyxy: tensor([[288.3297, 141.1603, 328.3002, 197.8108]])\n",
            "xyxyn: tensor([[0.2816, 0.3610, 0.3206, 0.5059]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.5117])\n",
            "data: tensor([[6.3968e+02, 2.0682e+02, 6.8360e+02, 3.3654e+02, 5.1168e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[661.6385, 271.6783,  43.9166, 129.7247]])\n",
            "xywhn: tensor([[0.6461, 0.6948, 0.0429, 0.3318]])\n",
            "xyxy: tensor([[639.6803, 206.8160, 683.5969, 336.5407]])\n",
            "xyxyn: tensor([[0.6247, 0.5289, 0.6676, 0.8607]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.5116])\n",
            "data: tensor([[8.6746e+02, 2.2461e+02, 9.1906e+02, 3.3730e+02, 5.1163e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[893.2639, 280.9518,  51.5980, 112.6869]])\n",
            "xywhn: tensor([[0.8723, 0.7185, 0.0504, 0.2882]])\n",
            "xyxy: tensor([[867.4648, 224.6083, 919.0629, 337.2952]])\n",
            "xyxyn: tensor([[0.8471, 0.5744, 0.8975, 0.8626]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.4446])\n",
            "data: tensor([[9.0958e+02, 2.1414e+02, 9.7458e+02, 3.3792e+02, 4.4456e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[942.0791, 276.0320,  64.9973, 123.7797]])\n",
            "xywhn: tensor([[0.9200, 0.7060, 0.0635, 0.3166]])\n",
            "xyxy: tensor([[909.5804, 214.1422, 974.5778, 337.9219]])\n",
            "xyxyn: tensor([[0.8883, 0.5477, 0.9517, 0.8643]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.3702])\n",
            "data: tensor([[5.5363e+02, 1.7862e+02, 5.8626e+02, 3.2945e+02, 3.7017e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[569.9438, 254.0350,  32.6321, 150.8362]])\n",
            "xywhn: tensor([[0.5566, 0.6497, 0.0319, 0.3858]])\n",
            "xyxy: tensor([[553.6278, 178.6169, 586.2599, 329.4531]])\n",
            "xyxyn: tensor([[0.5407, 0.4568, 0.5725, 0.8426]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.3331])\n",
            "data: tensor([[8.6609e+02, 2.0546e+02, 9.3080e+02, 3.3565e+02, 3.3309e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[898.4412, 270.5555,  64.7090, 130.1845]])\n",
            "xywhn: tensor([[0.8774, 0.6920, 0.0632, 0.3330]])\n",
            "xyxy: tensor([[866.0867, 205.4632, 930.7957, 335.6477]])\n",
            "xyxyn: tensor([[0.8458, 0.5255, 0.9090, 0.8584]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.3291])\n",
            "data: tensor([[4.8746e+02, 1.8828e+02, 5.1152e+02, 3.1912e+02, 3.2911e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[499.4937, 253.7030,  24.0598, 130.8388]])\n",
            "xywhn: tensor([[0.4878, 0.6489, 0.0235, 0.3346]])\n",
            "xyxy: tensor([[487.4639, 188.2836, 511.5236, 319.1224]])\n",
            "xyxyn: tensor([[0.4760, 0.4815, 0.4995, 0.8162]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.3057])\n",
            "data: tensor([[6.0406e+02, 2.4906e+02, 6.5177e+02, 3.3761e+02, 3.0572e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[627.9127, 293.3340,  47.7143,  88.5560]])\n",
            "xywhn: tensor([[0.6132, 0.7502, 0.0466, 0.2265]])\n",
            "xyxy: tensor([[604.0555, 249.0560, 651.7698, 337.6120]])\n",
            "xyxyn: tensor([[0.5899, 0.6370, 0.6365, 0.8635]])\n",
            "ultralytics.engine.results.Boxes object with attributes:\n",
            "\n",
            "cls: tensor([0.])\n",
            "conf: tensor([0.2702])\n",
            "data: tensor([[9.1430e+01, 1.6962e+02, 1.2725e+02, 3.1311e+02, 2.7022e-01, 0.0000e+00]])\n",
            "id: None\n",
            "is_track: False\n",
            "orig_shape: (391, 1024)\n",
            "shape: torch.Size([1, 6])\n",
            "xywh: tensor([[109.3379, 241.3658,  35.8167, 143.4950]])\n",
            "xywhn: tensor([[0.1068, 0.6173, 0.0350, 0.3670]])\n",
            "xyxy: tensor([[ 91.4296, 169.6183, 127.2462, 313.1133]])\n",
            "xyxyn: tensor([[0.0893, 0.4338, 0.1243, 0.8008]])\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n",
        "model.predict(source=\"0\", show=True, stream=True, classes=0)  # [0, 3, 5] for multiple classes\n",
        "\n",
        "\n",
        "# Define image file path\n",
        "image_folder = 'C:\\\\DEV\\\\YOLOv8Pro\\\\train_data\\\\'\n",
        "\n",
        "# Define image file names\n",
        "image_file_1 = '20190621_092709.png'\n",
        "image_file_2 = '23ppiicc170425.png'\n",
        "\n",
        "# Construct full image file paths\n",
        "image_path_1 = image_folder + image_file_1\n",
        "image_path_2 = image_folder + image_file_2\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "results = model([image_path_1, image_path_2])  # return a list of Results objects\n",
        "model.predict(source=\"0\", show=True, stream=True, classes=0)\n",
        "\n",
        "# Process results list\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs\n",
        "\n",
        "    # Print box coordinates\n",
        "    print(\"Box coordinates:\")\n",
        "    for box in boxes:\n",
        "        print(box)\n",
        "\n",
        "    # Display result with boxes\n",
        "    result.plot()\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f61191-a130-43fc-ae44-20d92fa6de0a",
      "metadata": {
        "id": "38f61191-a130-43fc-ae44-20d92fa6de0a"
      },
      "source": [
        "# 특정 사물만 인식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a42e48-0998-4f1e-9363-0818072b822e",
      "metadata": {
        "id": "e5a42e48-0998-4f1e-9363-0818072b822e"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n",
        "model.predict(source=\"0\", show=True, stream=True, classes=0)  # [0, 3, 5] for multiple classes\n",
        "\n",
        "# Define image file path\n",
        "image_folder = 'C:\\\\DEV\\\\SamsungElectronics_Gumi\\\\datasets\\\\YOLO\\\\'\n",
        "\n",
        "# Define image file names\n",
        "image_file_1 = 'im1.jpg'\n",
        "image_file_2 = 'im2.jpg'\n",
        "\n",
        "# Construct full image file paths\n",
        "image_path_1 = image_folder + image_file_1\n",
        "image_path_2 = image_folder + image_file_2\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "results = model([image_path_1, image_path_2])  # return a list of Results objects\n",
        "\n",
        "# Process results list\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs\n",
        "\n",
        "    # Print box coordinates\n",
        "    print(\"Box coordinates:\")\n",
        "    for box in boxes:\n",
        "        print(box)\n",
        "\n",
        "    # Display result with boxes\n",
        "    result.plot()\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코드 개선"
      ],
      "metadata": {
        "id": "6ek6glNiBHLp"
      },
      "id": "6ek6glNiBHLp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5a4a7d-da27-40c2-a64a-3a413c5a1324",
      "metadata": {
        "id": "fb5a4a7d-da27-40c2-a64a-3a413c5a1324"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pretrained YOLO model\n",
        "model = YOLO(\"yolov8n.pt\")  # COCO dataset으로 pretrained된 모델을 불러옴\n",
        "\n",
        "# Define image file path\n",
        "image_path = \"C:\\\\DEV\\\\SamsungElectronics_Gumi\\\\datasets\\\\YOLO\\\\im2.jpg\"\n",
        "\n",
        "# Use the model to perform object detection\n",
        "results = model.predict(image_path)\n",
        "\n",
        "# Print results for debugging\n",
        "print(results[0])\n",
        "print()\n",
        "print(results[0].boxes)  # 좌상단 좌표, 우하단 좌표, confidence score, class id\n",
        "\n",
        "# Plot the result\n",
        "res_plotted = results[0].plot()  # 결과 이미지를 그립니다 (bounding box 등)\n",
        "\n",
        "# Convert BGR (OpenCV format) to RGB (matplotlib format)\n",
        "res_plotted_rgb = cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(res_plotted_rgb)\n",
        "plt.axis('off')  # 축을 숨깁니다\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLOv8 커스텀 데이터 학습하기"
      ],
      "metadata": {
        "id": "hiDqLJPfBlWR"
      },
      "id": "hiDqLJPfBlWR"
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('C:\\\\DEV\\\\PycharmProjects\\\\ai_analysis\\\\YOLOv8Pro\\\\runs\\\\detect\\\\train19\\\\weights\\\\best.pt')  # pretrained YOLOv8n model\n",
        "\n",
        "# Define image file path\n",
        "image_folder = 'C:\\\\DEV\\\\PycharmProjects\\\\ai_analysis\\\\YOLOv8Pro\\\\images\\\\'\n",
        "\n",
        "# Define image file names\n",
        "image_file_1 = '51df913aec6e6cf119d01ca48a9d6664-770xAAA.png'\n",
        "image_file_2 = '23ppiicc170425.png'\n",
        "\n",
        "# Construct full image file paths\n",
        "image_path_1 = image_folder + image_file_1\n",
        "image_path_2 = image_folder + image_file_2\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "results = model([image_path_1, image_path_2])  # return a list of Results objects\n",
        "\n",
        "# Process results list\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs"
      ],
      "metadata": {
        "id": "oRVs3GeNBmoZ"
      },
      "id": "oRVs3GeNBmoZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# Load a model\n",
        "model = YOLO('C:\\\\DEV\\\\PycharmProjects\\\\ai_analysis\\\\YOLOv8Pro\\\\runs\\\\detect\\\\train19\\\\weights\\\\best.pt')  # pretrained YOLOv8n model\n",
        "\n",
        "model.predict(source=\"0\", show=True, stream=True, classes=0)  # [0, 3, 5] for multiple classes\n",
        "\n",
        "\n",
        "# Define image file path\n",
        "image_folder = 'C:\\\\DEV\\\\PycharmProjects\\\\ai_analysis\\\\YOLOv8Pro\\\\images\\\\'\n",
        "\n",
        "# Define image file names\n",
        "image_file_1 = '51df913aec6e6cf119d01ca48a9d6664-770xAAA.png'\n",
        "image_file_2 = '23ppiicc170425.png'\n",
        "\n",
        "# Construct full image file paths\n",
        "image_path_1 = image_folder + image_file_1\n",
        "image_path_2 = image_folder + image_file_2\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "results = model([image_path_1, image_path_2])  # return a list of Results objects\n",
        "model.predict(source=\"0\", show=True, stream=True, classes=0)\n",
        "\n",
        "# Process results list\n",
        "for result in results:\n",
        "    boxes = result.boxes  # Boxes object for bbox outputs\n",
        "    masks = result.masks  # Masks object for segmentation masks outputs\n",
        "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
        "    probs = result.probs  # Class probabilities for classification outputs\n",
        "\n",
        "    # Print box coordinates\n",
        "    print(\"Box coordinates:\")\n",
        "    for box in boxes:\n",
        "        print(box)\n",
        "\n",
        "    # Display result with boxes\n",
        "    result.plot()\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "X1Tef0HSBzM5"
      },
      "id": "X1Tef0HSBzM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tz8KFA3uBl96"
      },
      "id": "Tz8KFA3uBl96"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py3_10_yolov8",
      "language": "python",
      "name": "py3_10_yolov8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}